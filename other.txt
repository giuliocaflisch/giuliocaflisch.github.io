<!--
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <link href="static/structures/bootstrap/bootstrap.min.css" rel="stylesheet">
        <link href="static/style.css" rel="stylesheet">

        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>

    <body>
        <title>Math Diary</title>

        <div class="container-fluid">
            <div class="row">
                <div class="col">
                    <h1><center><u>Diario Matematico</u></center></h1>
                    <hr>

                    <h2>• Analisi scalare, infininitesimale e finitesimale</h2>
                    Magari ti è capitato di notare l'immensa somiglianza tra il l'analisi infinitesimale e finitesimale.
                    La domanda che sorge spontanea è quanto sia possibile estendere la costruzione sintattica e semantica degli operatori seguenti in una maniera tale da preservare il maggior numero di analogie tra essi.
                    \begin{align*}
                        \textcolor{blue}{D}, &\textcolor{blue}{\int \cdot d} \\
                        \textcolor{red}{\Delta}, &\textcolor{red}{\sum \cdot \delta}
                    \end{align*}
                    <br>
                    <br>
                    <h3>Definizioni</h3>
                    <div class="container">
                        <div class="row mt">
                            <div class="col">
                                <h3>Derivate</h3>
                                Per quanto riguarda le derivate è spontaneo definirla in termini del rapporto incrementale.
                                Nel caso infinitesimo si considera quindi il limite di tale rapporto.
                                In un linguaggio puramente formale, come quello fornitoci dal paradigma di programmazione funzionale, la definizione sarebbe la seguente.
                                \begin{align*}
                                    \textcolor{blue}{D_{_0}} &:= \\
                                        & f \mapsto \left(x \mapsto \lim_{\textcolor{blue}{h} \to \textcolor{blue}{0}} \frac{f(x+\textcolor{blue}{h}) - f(x)}{\textcolor{blue}{h}} \right) \\
                                    \textcolor{red}{\Delta_{_h}} &:= \\
                                        & f \mapsto \left(x \mapsto \frac{f(x+\textcolor{red}{h}) - f(x)}{\textcolor{red}{h}}\right)
                                \end{align*}
                                In pratica la derivata è definita come un'operatore che prende come argomento una funzione e restituisce come risultato la funzione del rapporto incrementale dell'argomento.
                                <br>
                                <br>
                                Utilizziamo inoltre associatività da sinistra verso destra in quanto la derivata di una funzione in uno specifico punto applica dapprima sulla funzione restituendo una nuova funzione, quest'ultima è poi analizzata nel punto specificato.
                                \begin{align*}
                                    \textcolor{blue}{D_{_0}} f(x) &= \left(\textcolor{blue}{D_{_0}} f\right)(x) \\
                                    \textcolor{red}{\Delta_{_h}} f(x) &= \left(\textcolor{red}{\Delta_{_h}} f\right)(x)
                                \end{align*}
                                Se così non fosse, e considerassimo la derivata con associatività contraria, non avremmo la "derivata di una funzione" il cui risultato è calcolato in un "particolare punto" come sperato.
                                Avremmo invece la "derivata" di "una funzione già calcolata in un particolare punto", che sarebbe decisamente sbagliato.
                                <br>
                                <br>
                                
                                E' possibile immaginare di definire, oltre all'operatore di derivata infinitesima e finitesima, anche gli operatori di differenziali totali finitesimi e infinitesimi (in effetti molti defniscono questi nel caso fnintesimo con lettere maiuscole, cosa che mi infastidisce alquanto).
                                Fare ciò nel caso finitesimo è semplice. Decisamente meno per l'inifinitesimo, dove serve costruire l'enorme teoreia a me sconosciuta della geometria differenziale.
                                <br>
                                <br>

                                <h3>Integrali (Riemann)</h3>
                                Definiamo gli integrali nella maniera classica.
                                Ricordiamo che, per l'integrale infinitesimo, esiste una generalizzazione dovuta al matematico Lebesgue che permette di integrare che, secondo la definizione corrente sono prive di senso, tramite la teoria della misura.
                                <br>
                                Per quanto riguarda l'integrale finitesimo, invece, è meglio definirlo a piccoli passi.
                                Prima estendiamo la sommatoria standard con estremi interi, per traslazione, alla sommatoria con estremi a distanza multiplo intero dello step di analisi.
                                Il caso infinitesimo non è affetto da considerazioni di distanza tra gli estremi in quanto, intuitivamente, ogni due numeri reali risiedono a distanza che è multiplo (infinito) dello step (infinitesimo).
                                Omettiamo le variabili di cammino quando possibile per semplificare la notazione.
                                \begin{align*}
                                    \textcolor{blue}{_{_0} \!\! \int}_{x_0}^{x_1 \textcolor{blue}{- 0}} f \textcolor{blue}{\cdot d_{_0}} &:= \lim_{\textcolor{blue}{h} \to \textcolor{blue}{0}} \textcolor{white}{\sum}_{0}^{\frac{x_1 - x_0}{\textcolor{blue}{h}} \textcolor{white}{- 1}} f(x_0 + \textcolor{blue}{h} \cdot n) \cdot \textcolor{blue}{h} \textcolor{white}{\cdot \delta} n \\
                                    \textcolor{red}{_{_h} \! \sum}_{x_0}^{x_1 \textcolor{red}{- h}} f \textcolor{red}{\cdot \delta_{_h}} &:= \textcolor{white}{\sum}_{0}^{\frac{x_1 - x_0}{\textcolor{red}{h}} \textcolor{white}{- 1}} f(x_0 + \textcolor{red}{h} \cdot n) \cdot \textcolor{red}{h} \textcolor{white}{\cdot \delta} n
                                \end{align*}
                                Anche qui, come già effettuato per le derivate, estendiamo la definizione per accettare espressioni qualsiasi come argomento di un integrale (sempre se si esplicita la variabile nella quale l'espressione è vista come funzione).
                                \begin{align*}
                                    \textcolor{blue}{_{_0} \!\! \int}_{x_0}^{x_1 \textcolor{blue}{- 0}} expr(x, y, z, \dots) \textcolor{blue}{\cdot d_{_0}} x &= \textcolor{blue}{_{_0} \!\! \int}_{x_0}^{x_1 \textcolor{blue}{- 0}} (x' \mapsto expr(x', y, z, \dots)) \textcolor{blue}{\cdot d_{_0}} \\
                                    \textcolor{red}{_{_h} \! \sum}_{x_0}^{x_1 \textcolor{red}{- h}} expr(x, y, z, \dots) \textcolor{red}{\cdot \delta_{_h}} x &= \textcolor{red}{_{_h} \! \sum}_{x_0}^{x_1 \textcolor{red}{- h}} (x' \mapsto expr(x', y, z, \dots)) \textcolor{red}{\cdot \delta_{_h}}
                                \end{align*}
                                Bisogna precisare che la variabile di cammino nell'integrale non ha nulla a che fare definizionalmente con la variabile nella quale si esprime il risultato (che dipende dalle variabili poste agli estremi).
                                L'integrale è un operatore non locale in quanto necessita di due estremi per restituire un risultato.
                                <br>
                                Inoltre utilizzeremo la convezione della scrittura di integrali indefiniti, per i quali si omettono gli estremi di integrazione specificando o meno la variabile di integrazione.
                                <br>
                                <br>
                                Useremo la convenzione di scrivere gli estremi decrementati dello step (poichè semplifica pressochè tutti i teoremi) e quella di quasi-inversione degli estremi in ordine decrescente che segue (dato che questa è l'unica che preserva sia la somma triviale uguale a 0 per estremi uguali a meno di decremento e la concatenazione di intervalli).
                                \begin{align*}
                                    : \textcolor{blue}{_{_0} \!\! \int}_{x_0}^{x_1 \textcolor{blue}{- 0}} f \textcolor{blue}{\cdot d_{_0}} &= 0 - \textcolor{blue}{_{_0} \!\! \int}_{x_1}^{x_1 \textcolor{blue}{- 0}} f \textcolor{blue}{\cdot d_{_0}} \\
                                    : \textcolor{blue}{_{_0} \!\! \int}_{x_0}^{x_0 \textcolor{blue}{- 0}} f \textcolor{blue}{\cdot d_{_0}} &= 0 \\
                                    \\
                                    : \textcolor{red}{_{_h} \! \sum}_{x_0}^{x_1 \textcolor{red}{- h}} f \textcolor{red}{\cdot \delta_{_h}} &= 0 - \textcolor{red}{_{_h} \! \sum}_{x_1}^{x_0 \textcolor{red}{- h}} f \textcolor{red}{\cdot \delta_{_h}} \\
                                    : \textcolor{red}{_{_h} \!\sum}_{x_0}^{x_0 \textcolor{red}{- h}} f \textcolor{red}{\cdot \delta_{_h}} &= 0
                                \end{align*}
                            </div>
                        </div>
                    </div>
                    
                    <h3>Teorema fondamentale</h3>
                    Non c'è molto da dire in più di quanto non si possa trovare in ogni altro libro di analisi.
                    Bisogna ricordarsi che, in particolare nel caso infinitesimo, la funzione integrata e derivata (o viceversa) deve soddisfare alcune ipotesi di regolarità affinchè l'identità scritta valga (e.g. continua o derivabile con derivata continua rispettivamente).
                    <div class="container">
                        <div class="row">
                            <div class="col">
                                <h3>Derivata dell'integrale</h3>
                                \begin{align*}
                                    \textcolor{blue}{D_{_0}} \left(x \mapsto \textcolor{blue}{_{_0} \!\! \int}_{x_0}^{x \textcolor{blue}{- 0}} f \textcolor{blue}{\cdot d_{_0}} \right) &= f \\
                                    \textcolor{red}{\Delta_{_h}} \left(x \mapsto \textcolor{red}{_{_h} \! \sum}_{x_0}^{x \textcolor{red}{- h}} f \textcolor{red}{\cdot \delta_{_h}} \right) &= f
                                \end{align*}

                                <h3>Integrale della derivata</h3>
                                \begin{align*}
                                    \textcolor{blue}{_{_0} \!\! \int}_{x_0}^{x_1 \textcolor{blue}{- 0}} \textcolor{blue}{D_{_0}} f \textcolor{blue}{\cdot d_{_0}} &= f(x_1) - f(x_0) \\
                                    \textcolor{red}{_{_h} \! \sum}_{x_0}^{x_1 \textcolor{red}{- h}} \textcolor{red}{\Delta_{_h}} f \textcolor{red}{\cdot \delta_{_h}} &= f(x_1) - f(x_0)
                                \end{align*}

                                <h3>Unicità della primitiva su intervall a meno di ...</h3>
                                ...
                            </div>
                        </div>
                    </div>
                    
                    <h3>Calcolo</h3>
                    Le formule elencate in seguito sono scritte in maniera tale da evidenziare le analogie al massimo e lasciar intuire come generalizzarle per step finitesimo arbitrario.
                    Per questo non sono ridotte ai minimu termini utilizzando proprietà specifiche degli step 0 e 1 presi in esame.
                    <div class="container">
                        <div class="row">
                            <div class="col">
                                <h3>Derivate</h3>

                                <h4>Costante</h4>
                                \begin{align*}
                                    k(\cdot) = k \Rightarrow & \; \textcolor{blue}{D_{_0}} k = 0 \\
                                        & \; \textcolor{red}{\Delta_{_h}} k = 0
                                \end{align*}

                                <h4>Addizione</h4>
                                \begin{align*}
                                    \textcolor{blue}{D_{_0}} (f + g) &= \textcolor{blue}{D_{_0}} f + \textcolor{blue}{D_{_0}} g \\
                                    \textcolor{red}{\Delta_{_h}} (f + g) &= \textcolor{red}{\Delta_{_h}} f + \textcolor{red}{\Delta_{_h}} g
                                \end{align*}

                                <h4>Moltiplicazione</h4>
                                \begin{align*}
                                    \textcolor{blue}{D_{_0}} \left(f \cdot g\right) &= \textcolor{blue}{D_{_0}} f \cdot g + f \cdot \textcolor{blue}{D_{_0}} g + \textcolor{blue}{0} \cdot \textcolor{blue}{D_{_0}} f \cdot \textcolor{blue}{D_{_0}} g \\
                                    \textcolor{red}{\Delta_{_h}} \left(f \cdot g\right) &= \textcolor{red}{\Delta_{_h}} f \cdot g + f \cdot \textcolor{red}{\Delta_{_h}} g + \textcolor{red}{h} \cdot \textcolor{red}{\Delta_{_h}} f \cdot \textcolor{red}{\Delta_{_h}} g
                                \end{align*}

                                <h4>Inverso moltiplicativo</h4>
                                \begin{align*}
                                    \textcolor{blue}{D_{_0}} (f \cdot^{-1}) &= - \frac{\textcolor{blue}{D_{_0}} f}{f \cdot (f + \textcolor{blue}{0} \cdot \textcolor{blue}{D_{_0}} f)} \\
                                    \textcolor{red}{\Delta_{_h}} (f \cdot^{-1}) &= - \frac{\textcolor{red}{\Delta_{_h}} f}{f \cdot (f + \textcolor{red}{h} \cdot \textcolor{red}{\Delta_{_h}} f)} \\
                                \end{align*}
                                <br>
                                Da cui seguono direttamente le regole dell'inverso additivo, della sottrazione, del prodotto per una costante e del rapporto.
                                <br>
                                <br>

                                <h4>Composizione (Leibniz)</h4>
                                \begin{align*}
                                    \textcolor{blue}{D_{_0}} (f \circ g) &= \textcolor{blue}{D_{_{0 \cdot D_{_0} g}}} f(g) \cdot \textcolor{blue}{D_{_0}} g \\
                                        &= \textcolor{blue}{D_{_0}} f(g) \cdot \textcolor{blue}{D_{_0}} g \\
                                    \\
                                    \textcolor{red}{\Delta_{_h}} (f \circ g) &= \textcolor{red}{\Delta_{_{h \cdot \Delta_{_h} g}}} f(g) \cdot \textcolor{red}{\Delta_{_h}} g \\
                                \end{align*}
                                
                                <h4>Inverso Compositivo (Leibniz)</h4>
                                \begin{align*}
                                    \textcolor{blue}{D_{_0}} (f \circ^{-1}) &= \frac{1}{\textcolor{blue}{D_{_{0 \cdot D_{_0} f \circ^{-1}}}} f(f \circ^{-1})} \\
                                        &= \frac{1}{\textcolor{blue}{D_{_0}} f(f \circ^{-1})} \\
                                    \\
                                    \textcolor{red}{\Delta_{_h}} (f \circ^{-1}) &= \frac{1}{\textcolor{red}{\Delta_{_{h \cdot \Delta_{_h} f \circ^{-1}}}} f(f \circ^{-1})} \\
                                \end{align*}
                                A lungo ho creduto che non esistesse una regola analoga a quella della derivata composta per la derivata discreta.
                                In un certo senso è vero.
                                Non è possibile, per un fissato step infinitesimo constante, riscrivere la derivata discreta della composizione in termini delle derivate discrete a quel dato step di entrambe.
                                L'unica strada possibile è quella di accettare che lo step di derivazione possa non essere lo stesso scomponendo e slegando le due funzioni e possa dipendere dal punto di derivazione.
                                Soprendentemente esiste così una formula per la derivata discreta della composizione ma è molto più brutta di quella per la derivata continua.
                                In quella continua, dato che il nuovo step risultante dalla variazione infinitesima della variabile primordiale induce uno step a sua volta infintesimo a sequito dell'applicazione alla funzione interna, possiamo considerare il nuovo step di variazione interno come pure infinitesimo alla pari.
                                Questo è ovviamente impossibile nel caso finitesimo dato che, una variazione fissa nell'argomento applicata alla funzione interna, genera una variazione distinta completamente distinta.
                                \begin{align*}
                                    \frac{\textcolor{blue}{d_{_0}}f(g(x))}{\textcolor{blue}{d_{_0}}x} &=^? \frac{\textcolor{blue}{d_{_0}}f(g(x))}{\textcolor{blue}{d_{_0}}g(x)} \cdot \frac{\textcolor{blue}{d_{_0}}g(x)}{\textcolor{blue}{d_{_0}}x} \\
                                            & \quad \quad =^? \left( \frac{\textcolor{blue}{d_{_0}}f(y)}{\textcolor{blue}{d_{_0}}y} \right)_{y = g(x)} \cdot \frac{\textcolor{blue}{d_{_0}}g(x)}{\textcolor{blue}{d_{_0}}x} \\
                                        &=^? \left( \frac{\textcolor{blue}{d_{_{d_{_0}g(x)}}}f(y)}{\textcolor{blue}{d_{_{d_{_0}g(x)}}}y} \right)_{y = g(x)} \cdot \frac{\textcolor{blue}{d_{_0}}g(x)}{\textcolor{blue}{d_{_0}}x} \\
                                        &=^? \left( \frac{\textcolor{blue}{d_{_0}}f(y)}{\textcolor{blue}{d_{_0}}y} \right)_{y = g(x)} \cdot \frac{\textcolor{blue}{d_{_0}}g(x)}{\textcolor{blue}{d_{_0}}x} \\
                                    \\
                                    \frac{\textcolor{red}{\delta_{_h}}f(g(x))}{\textcolor{red}{\delta_{_h}}x} &=^! \frac{\textcolor{red}{\delta_{_h}}f(g(x))}{\textcolor{red}{\delta_{_h}}g(x)} \cdot \frac{\textcolor{red}{\delta_{_h}}g(x)}{\textcolor{red}{\delta_{_h}}x} \\
                                            & \quad \quad \cancel{=^? \left( \frac{\textcolor{red}{\delta_{_h}}f(y)}{\textcolor{red}{\delta_{_h}}y} \right)_{y = g(x)} \cdot \frac{\textcolor{red}{\delta_{_h}}g(x)}{\textcolor{red}{\delta_{_h}}x}} \\
                                        &=^! \left( \frac{\textcolor{red}{\delta_{_{\delta_{_h} g(x)}}}f(y)}{\textcolor{red}{\delta_{_{\delta_{_h} g(x)}}}y} \right)_{y = g(x)} \cdot \frac{\textcolor{red}{\delta_{_h}}g(x)}{\textcolor{red}{\delta_{_h}}x}
                                \end{align*}
                                Ovviamente questa formula ha soltanto senso quando il nuovo differenziale generato dalla funzione interna è diverso da 0.
                                Nel caso in cui la funzione interna non varia ne segue ovviamente che anche quella esterna non subisce a cascata variazione.
                                Quindi, volendo, la derivata interna moltiplicata da sinistra di valore 0 risolve anche questo caso passando al limite la derivata discreta (che tanto viene annullata).
                                <br>
                                <br>

                                <h4>Potenze decrementative (Pochhammer)</h4>
                                Dobbiamo prima definire le potenze decrementative nel caso di esponente intero.
                                Per fare ciò generalizziamo la produttoria con estremi invertiti, per adesso solo nel caso di estremi a distanza intera, nella maniera analoga a quanto fatto per la sommatoria.
                                \begin{align*}
                                    \textcolor{white}{\prod}_{x_0}^{x_1 - 1} f \textcolor{white}{\cdot}^{\textcolor{white}{\delta}} = 1 / \left( \textcolor{white}{\prod}_{x_1}^{x_0 - 1} f \textcolor{white}{\cdot}^{\textcolor{white}{\delta}} \right)\\
                                \end{align*}
                                Lo ripeto anche qui per essre chiaro.
                                Capisco che questa estensione della produttoria può sembrare strana in un primo momento ma è l'unica che preserva la concatenzaione di intervalli (e.g. prodotto da 1 a 5 per prodotto da 5 a 1 che procede e risulta all'inverso) e la produttoria vuota per estremi uguali a meno di decremento.
                                <br>
                                <br>
                                Ecco qui la super concisa definizione data dal lavoro precedente.
                                \begin{align*}
                                    (x) \cdot^n_{\textcolor{blue}{-0}} &:= \textcolor{white}{\prod}_{0}^{n - 1} (x - k \cdot \textcolor{blue}{0}) \textcolor{white}{\cdot}^{\textcolor{white}{\delta} k} \\
                                    (x) \cdot^n_{\textcolor{red}{-h}} &:= \textcolor{white}{\prod}_{0}^{n - 1} (x - k \cdot \textcolor{red}{h}) \textcolor{white}{\cdot}^{\textcolor{white}{\delta} k}
                                \end{align*}
                                Possiamo notare che, per esponente negatico, moltiplichiamo al denominatore potenze incrementative partendo dall'incremento di uno step (e non di 0 come per esponente positivo).
                                <br>
                                <br>
                                Sembra incredibile ma questa, per definizione con esponenti interi, vale la classica regola di derivazione delle potenze scritta sotto. 
                                \begin{align*}
                                    \textcolor{blue}{D_{_0}} \left(x \mapsto (x) \cdot^n_{\textcolor{blue}{-0}} \right) &= (x \mapsto n \cdot (x)\cdot^{n - 1}_{\textcolor{blue}{-0}}) \\
                                    \textcolor{red}{\Delta_{_h}} \left(x \mapsto (x) \cdot^n_{\textcolor{red}{-h}} \right) &= (x \mapsto n \cdot (x)\cdot^{n - 1}_{\textcolor{red}{-h}})
                                \end{align*}
                                Ancor più entusiasmante è il fatto che esiste un'estensione per esponente complesso generico usando la continuazione analitica del fattoriale (che io indico con la lettera greca Digamma).
                                \begin{align*}
                                    (x) \cdot^n_{\textcolor{blue}{- 0}} &:= \lim_{\textcolor{blue}{h} \rightarrow \textcolor{blue}{0}} \textcolor{blue}{h} \cdot^{n} \cdot \frac{\digamma\left(\frac{x}{\textcolor{blue}{h}}\right)}{\digamma\left(\frac{x}{\textcolor{blue}{h}} - n \right)} \\
                                    (x) \cdot^n_{\textcolor{red}{- h}} &:= \textcolor{red}{h} \cdot^{n} \cdot \frac{\digamma\left(\frac{x}{\textcolor{red}{h}}\right)}{\digamma\left(\frac{x}{\textcolor{red}{h}} - n \right)}
                                \end{align*}
                                Questa definizione non ha senso per esponente intero negativo nel caso a step decrementativo finitesimo uguale a 1.
                                Ma, sorprendentemente, la definizione precedente in questo caso ha perfettamente senso ed è proprio quella che si ottiene al limite come continuazione.
                                <br>
                                Ultima cosa, ma non per importanza, la regola della potenza continua a valere anche per esponenti non interi.
                                Questo fatto è estremamente eccitante!
                                <br>
                                <br>

                                <h4>Esponenziali elementari (Napier)</h4>
                                Prima definiamo le basi esponenziali elementari.
                                \begin{align*}
                                    \textcolor{blue}{e_{_0}} &:= \lim_{\textcolor{blue}{h} \rightarrow \textcolor{blue}{0}} (1 + \textcolor{blue}{h}) \cdot^{\frac{1}{\textcolor{blue}{h}}} \\
                                    \textcolor{red}{e_{_h}} &:= (1 + \textcolor{red}{h}) \cdot^{\frac{1}{\textcolor{red}{h}}}
                                \end{align*}
                                Ora possiamo analizzare le derivate.
                                \begin{align*}
                                    \textcolor{blue}{D_{_0}} ( x \mapsto \textcolor{blue}{e_{_0}} \cdot^x ) &= (x \mapsto \textcolor{blue}{e_{_0}} \cdot^x) \\
                                    \textcolor{red}{\Delta_{_h}} ( x \mapsto \textcolor{red}{e_{_h}} \cdot^x ) &= (x \mapsto \textcolor{red}{e_{_h}} \cdot^x)
                                \end{align*}
                                Notate qualcuno di familiare?
                                Ecco tra noi il numero di Napier!
                                In un certo senso, espresso da questa formula, il numero di Napier e due sono analoghi nel calcolo infinitesimo e finitesimo a step unitario rispettivamente.
                                <br>
                                <br>
                                
                                <h3>Integrali</h3>
                                Per alleggerire la notazione usiamo la convenzione per gli integrali indefiniti (lasciando però scritto il devrementare dell'estremo superiore per compatibilità e maggiore semplicità delle formule)
                                <br>
                                <br>
                                <h4>Costante</h4>
                                \begin{align*}
                                    k(\cdot) = k \Rightarrow \; \textcolor{blue}{_{_0} \!\! \int}_{}^{x \textcolor{blue}{- 0}} k \textcolor{blue}{\cdot d_{_0}} &= k \cdot x \\
                                        \textcolor{red}{_{_h} \! \sum}_{}^{x \textcolor{red}{- h}} k \textcolor{red}{\cdot \delta_{_h}} &= k \cdot x
                                \end{align*}

                                <h4>Addizione</h4>
                                \begin{align*}
                                    \textcolor{blue}{_{_0} \!\! \int}_{}^{x \textcolor{blue}{- 0}} \left(f + g\right) \textcolor{blue}{\cdot d_{_0}} &= \textcolor{blue}{_{_0} \!\! \int}_{}^{x \textcolor{blue}{- 0}} f \textcolor{blue}{\cdot d_{_0}} + \textcolor{blue}{_{_0} \!\! \int}_{}^{x \textcolor{blue}{- 0}} g \textcolor{blue}{\cdot d_{_0}} \\
                                    \textcolor{red}{_{_h} \! \sum}_{}^{x \textcolor{red}{- 1}} \left(f + g \right)\textcolor{red}{\cdot \delta_{_h}} &= \textcolor{red}{_{_h} \! \sum}_{}^{x \textcolor{red}{- h}} f \textcolor{red}{\cdot \delta_{_h}} + \textcolor{red}{_{_h} \! \sum}_{}^{x \textcolor{red}{- h}} g \textcolor{red}{\cdot \delta_{_h}}
                                \end{align*}
                                <br>

                                <h4>Per parti (dalla derivata della moltiplicazione)</h4>
                                \begin{align*}
                                    \textcolor{blue}{_{_0} \!\! \int}_{}^{x \textcolor{blue}{- 0}} f \cdot \textcolor{blue}{D_{_0}} g \textcolor{blue}{\cdot d_{_0}} &= f \cdot g - \textcolor{blue}{_{_0} \!\! \int}_{}^{x \textcolor{blue}{- 0}} \textcolor{blue}{D_{_0}} f \cdot g \textcolor{blue}{\cdot d_{_0}} - \textcolor{blue}{0} \cdot \textcolor{blue}{_{_0} \!\! \int}_{}^{x \textcolor{blue}{- 0}} \textcolor{blue}{D_{_0}} f \cdot \textcolor{blue}{D_{_0}} g \textcolor{blue}{\cdot d_{_0}} \\
                                    \textcolor{red}{_{_h} \!\! \sum}_{}^{x \textcolor{red}{- h}} f \cdot \textcolor{red}{\Delta_{_h}} g \textcolor{red}{\cdot \delta_{_h}} &= f \cdot g - \textcolor{red}{_{_h} \! \sum}_{}^{x \textcolor{red}{- h}} \textcolor{red}{\Delta_{_h}} f \cdot g \textcolor{red}{\cdot \delta_{_h}} - \textcolor{red}{h} \cdot \textcolor{red}{_{_h} \! \sum}_{}^{x \textcolor{red}{- h}} \textcolor{red}{\Delta_{_h}} f \cdot \textcolor{red}{\Delta_{_h}} g \textcolor{red}{\cdot \delta_{_h}}
                                \end{align*}
                                Da cui seguono direttamente le regole dell'inverso additivo, della sottrazione, e del prodotto per una costante.
                                <br>
                                <br>

                                <h4>Sostituzione (dalla derivata della composizione)</h4>
                                \begin{align*}
                                    \textcolor{blue}{_{_0} \!\! \int}_{x_0}^{x_1 \textcolor{blue}{- 0}} f \circ g \cdot \textcolor{blue}{D_{_0}} g \textcolor{blue}{\cdot d_{_0}} = \textcolor{blue}{_{_0} \!\! \int}_{g(x_0)}^{g(x_1) \textcolor{blue}{- 0}} f \textcolor{blue}{\cdot d_{_0}}
                                \end{align*}
                                La regola di sostituzione, in questa formulazione, esiste solo per l'analisi continua, ovviamente integrando a step finitesimo h entrambi i lati della derivata della composizione otteniamo una formulazione valida ma poco utile in pratica.
                                <br>

                                <h4>Potenze decrementative</h4>
                                Dobbiamo fare attenzione a dividere i casi in cui l'esponente è diverso o uguale a -1.
                                <br>
                                <b>Esponente diverso da -1</b>
                                \begin{align*}
                                    \textcolor{blue}{_{_0} \!\! \int}_{}^{x \textcolor{blue}{- 0}} (t) \cdot^n _{\textcolor{blue}{-0}} \textcolor{blue}{\cdot d_{_0}} t &= \frac{(x) \cdot^{n + 1}_{\textcolor{blue}{-0}}}{n + 1} \\
                                    \textcolor{red}{_{_h} \! \sum}_{}^{x \textcolor{red}{- 1}} (t) \cdot^n _{\textcolor{red}{-h}} \textcolor{red}{\cdot \delta_{_h}} t &= \frac{(x) \cdot^{n + 1}_{\textcolor{red}{-h}}}{n + 1}
                                \end{align*}
                                <b>Esponente uguale a -1</b>
                                \begin{align*}
                                    \textcolor{blue}{_{_0} \!\! \int}_{x_0}^{x_1 \textcolor{blue}{- 0}} (t) \cdot^{-1} _{\textcolor{blue}{-0}} \textcolor{blue}{\cdot d_{_0}} t &= \lim_{\textcolor{blue}{h} \rightarrow \textcolor{blue}{0}} \left(H\left(\frac{x_1}{\textcolor{blue}{h}}\right) - H\left(\frac{x_0}{\textcolor{blue}{h}}\right)\right)\\
                                        &= \ln(x_1) - \ln(x_0) \\
                                    \\
                                    \textcolor{red}{_{_0} \! \sum}_{x_0}^{x_1 \textcolor{red}{- h}} (t) \cdot^{-1} _{\textcolor{red}{-h}} \textcolor{red}{\cdot \delta_{_h}} t &= H\left(\frac{x_1}{\textcolor{red}{h}}\right) - H\left(\frac{x_0}{\textcolor{red}{h}}\right)
                                \end{align*}
                                Da notare come la potenza decrementativa a step 1 per esponente -1, abbia un incremento nel denominatore di 1 che è esattamente quello che serve per definire la serie armonica partendo da zero fino all'estremo decrementato!
                                Coincidenze? Io non credo.
                                <br>
                                <br>

                                <h4>Esponenziali elementari</h4>
                                \begin{align*}
                                    \textcolor{blue}{_{_0} \!\! \int}_{}^{x \textcolor{blue}{- 0}} \textcolor{blue}{e_{_0}} \cdot^x \textcolor{blue}{\cdot d_{_0}} t &= \textcolor{blue}{e_{_0}} \cdot^x \\
                                    \textcolor{red}{_{_h} \! \sum}_{}^{x \textcolor{red}{- h}} \textcolor{red}{e_{_h}} \cdot^x \textcolor{red}{\cdot \delta_{_h}} t &= \textcolor{red}{e_{_h}} \cdot^x
                                \end{align*}
                            </div>
                        </div>
                    </div>

                    <h3>Estensione</h3>
                    <div class="container">
                        <div class="row">
                            <div class="col">
                                <h3>Derivate</h3>
                                ...
                                <br>
                                <br>

                                <h3>Integrali</h3>
                                <h4>Teorema di compatibilità (Euler e Maclaurin)</h4>
                                \begin{align*}
                                    \textcolor{red}{_{_h} \; \sum}_{x_0}^{x_1 \textcolor{red}{- h}} f \textcolor{red}{\cdot \delta_{_h}} = \textcolor{blue}{_{_0} \!\! \int}_{x_0}^{x_1 \textcolor{blue}{- 0}} f \textcolor{blue}{\cdot d_{_0}} + \textcolor{white}{\sum}_{0}^{\infty - 1} \frac{B(k + 1)}{\digamma(x + 1)} \cdot \textcolor{red}{h} \cdot^{k+1} \cdot (\textcolor{blue}{D} \circ^k f(x_1) - \textcolor{blue}{D} \circ^k f(x_0)) \textcolor{white}{\cdot \delta} k
                                \end{align*}
                                L'ugualianza vale nel caso in cui la serie converge e, ovviamente, ha senso solo se la funzione sia derivabile infinitesimamente un numero arbitrario di volte.
                                Guardando alla formula osserviamo che, sul lato sinistro, l'ipotesi che gli estremi siano distanti tra loro un numero intero non è utilizza.
                                Questo ci invita a pensare che, magari, questa proposizione (che è un teorema nel caso di estremi a distanza intera), possa essere considerato una definizione per estremi a distanza arbitraria (nel caso in cui la serie converga).
                                Immaginate per un attimo il potere di questa cosa.
                                Se fosse compatibile con con i teoremi scritti prima avermmo un integrale finitesimo con i superpoteri.
                                In effetti è proprio così (per funzioni abbastanza regolari dove questa definizione ha senso).
                                Questo è probabilmente il mio teorema preferito in tutta quanta la matematica.
                                In pratica esso permette di estendere la somma semanticamente nella maniera identica a come sintatticamente uno desidererebbe che sia.
                                I teoremi precedentemente descritti continuano a valere assunta la regolarità (devo studiare bene dove, come e quando fallisce).
                            </div>
                        </div>
                    </div>

                    <h3>Esercizi, potere e limitazione delle analogie</h3>
                    <div class="container">
                        <div class="row">
                            <div class="col">
                                <h3>Integrali</h3>
                                <h4>Polinomi in forma non decrementativa</h4>
                                \begin{align*}
                                    \textcolor{red}{\sum}_{0}^{x \textcolor{red}{- 1}} t \cdot^2 \textcolor{red}{\cdot \delta} t &= \textcolor{red}{\sum}_{0}^{x \textcolor{red}{- 1}} (t) \cdot^2 _{\textcolor{blue}{-0}} \textcolor{red}{\cdot \delta} t \\
                                        &= \textcolor{red}{\sum}_{0}^{\textcolor{red}{- 1}} \left( (t) \cdot^2 _{\textcolor{red}{-1}} \, - \; (t) \cdot^1 _{\textcolor{red}{-1}} \right) \textcolor{red}{\cdot \delta} t \\
                                        &= \left(\frac{(x) \cdot^3 _{\textcolor{red}{-1}}}{3} - \frac{(x) \cdot^2 _{\textcolor{red}{-1}}}{2} \right) - \cancel{\left(\frac{(0) \cdot^3 _{\textcolor{red}{-1}}}{3} - \frac{(0) \cdot^2 _{\textcolor{red}{-1}}}{2} \right)}\\
                                        &= \frac{(x) \cdot^3 _{\textcolor{blue}{-0}}}{3} - \frac{(x) \cdot^2 _{\textcolor{blue}{-0}}}{2} + \frac{(x) \cdot^1 _{\textcolor{blue}{-0}}}{6} \\
                                        &= \frac{x \cdot^3}{3} - \frac{x \cdot^2}{2} + \frac{x}{6}
                                \end{align*}
                                Questo esercizio mi ha insegnato due cose importanti.
                                La prima è che spesso in matematica è importante avere tante diverse basi di uno spazio (nel nostro caso lo spazio dei polinomi avente come basi distinte le potenze decrementative con step 0 e 1) e ricordarsi in quale caso una base conviene rispetto ad un'altra.
                                Ongi nuova base è un nuovo modo di vedere e interagire con l'oggetto matematico in questione.
                                La seconda è che un algoritmo vale 100 teoremi.
                                Cosa centra questo con l'esercizio di sopra vi starete chiedendo.
                                Mi ricordo che, durante la mia prima lezione di università, abbiamo dimostrato proprio questa formula ma in una maniera che necessita la conoscenza precedente di tale fatto (dimostrazione per ricorsione).
                                Tutti in classe ci stavamo chiedendo da dove spuntasse fuori questa formula, in effetti una volta che si conosce questo candidato non è difficile dimostrare che sia quello giusto.
                                La domanda è come trovare tale candidato.
                                La risposta che abbiamo ricevuto dal professore era che lo si può trovare sapendo che deve crescere come un polinomio di grado 3 (sì, ma ancora manca tanto prima di giungere a questo specifico polinomio).
                                Con questa risposta mi sono accorto che è possibile creare un algoritmo per riscrivere polinomi standard in polinomi decrementativi a step unitario equivalenti, per i quali è decisamente più facile risolvere l'integrale discreto.
                                In questo senso un algoritmo vale non solo 100 teoremi, ma infiniti contabili teoremi tirati dal cilindro (uno per ogni esponende della potenza non decrementativa).
                                Da qui la mia passione per le scienze formali computazionali che, sotto alcuni aspetti, supera la madre di tutte le scienze formali che è la matematica.
                                <br>
                                <br>

                                <h4>Potenze del logaritmo e della serie armonica</h4>
                                Per i curiosi ho calcolato (e invito a fare per conto vostro), l'integrale discreto delle prime potenze della serie arminica e l'ho messo a confronto con l'integrale continuo del logaritmo.
                                Come potete vedere l'estremo superiore decrementato fornisce grandi analogie ma, a causa del termine in più presente nell0integrazione per parti discreta, l'analogia si sgretola progressivamente.
                                \begin{align*}
                                    \textcolor{blue}{\int}_{0}^{x \textcolor{blue}{- 0}} \ln \textcolor{blue}{\cdot d} =& \; x \cdot \ln(x) - x \\
                                    \textcolor{red}{\sum}_{0}^{x \textcolor{red}{- 1}} H \textcolor{red}{\cdot \delta} =& \; x \cdot H(x) - x \\
                                    \\
                                    \textcolor{blue}{\int}_{0}^{x \textcolor{blue}{- 0}} \ln \cdot^2 \textcolor{blue}{\cdot d} =& \; x \cdot \ln(x) \cdot^2 - 2 \cdot x \cdot \ln(x) + 2 \cdot x \\
                                    \textcolor{red}{\sum}_{0}^{x \textcolor{red}{- 1}}  H \cdot^2 \textcolor{red}{\cdot \delta} =& \; x \cdot  H(x) \cdot ^2 - 2 \cdot x \cdot  H(x) + 2 \cdot x \\
                                    &\; \textcolor{grey}{- H(x)} \\
                                    \\
                                    \textcolor{blue}{\int}_{0}^{x \textcolor{blue}{- 0}} \ln \cdot^3 \textcolor{blue}{\cdot d} =& \; x \cdot \ln(x) \cdot^3 - 3 \cdot x \cdot \ln(x) \cdot ^2 + 6 \cdot x \cdot \ln(x) - 6 \cdot x\\
                                    \textcolor{red}{\sum}_{0}^{x \textcolor{red}{- 1}} H \cdot^3 \textcolor{red}{\cdot \delta} =& \; x \cdot H(x) \cdot^3 - 3 \cdot x \cdot H(x) \cdot^2 + 6 \cdot x \cdot H(x) - 6 \cdot  x \\
                                    & \; \textcolor{grey}{-\frac{3}{2} \cdot H\left(x\right) \cdot^{2} + 3 \cdot H\left(x\right) - \frac{1}{2} \cdot D H (x) + \frac{\pi^{2}}{12}} \\
                                    \\
                                    \dots
                                \end{align*}
                                Se l'ultima formula non vi sorprende allora non avete sentimenti alcuni.
                                Sono molto fiero della somma dei cubi della serie armonica poichè è stato uno dei primi casi nella mia vita dove i principali programmi di computazione simbolica non mi hanno dato una risposta esplicita mentre, con un quarto d'ora di calcolo seguendo le indicazioni scritte sopra, sono arrivato a una conclusione che mi ha sorpreso positivamente.
                                Come mai è spintato pi greco dal nulla? Cosa centra la derivata infinitesimale nella risoluzione di un integrale finitesimale?
                                Potete scoprirlo usando soltanto i teoremi elencati sopra definendo
                            </div>
                        </div>
                    </div>

                    <h3>Step arbitrario</h3>
                    Come potete notare ho messo in risalto con il colore componenti numeriche che permettono di visualizzare meglio come funzionerebbe una generalizzazione degli operatori analitici discreti per step incrementatico arbitrario.
                    Non sto qui a raccontarvi quanto questa teoria generalizzata sia importante per ogni tipo di simulazioni numeriche e studi di complessità asintotica.
                    Comunque voglio dire che, nonostante questa sia una formulazione personale della sintassi e semantica alla base dell'analisi, è pressocchè impossibile non notare la semplicità e la formalità dietro questa construzione.
                    Sono convinto che molto di quello che è scritto qui può essere trovato ovunque su internet ma, quasi sicuramente, necessiterebbe più tempo per coglierne direttamente la semplicità nascosta.
                    Questo è dovuto al fatto che i matematici non si dedicano a rendere più semplici alcuni concetti per i futuri loro pari.
                    Questa visione elitaria della conoscenza deve finire.
                    Non è vero che chi si ferma a semplificare il passato invece di correre in profondità è perduto.
                    Mi scuso per il finale poco matematico e più filosofico e spero che questi miei appunti vi possano essere utili.
                    <hr>

                    <h2>• Analisi vettoriale, finitesimale e infinitesimale</h2>
                    ...

                    <hr>

                </div>
            </div>
        </div>

        <script src="static/structures/bootstrap/bootstrap.bundle.min.js"></script>
        <script src="static/interactive.js"></script>
    </body> 
</html>
-->